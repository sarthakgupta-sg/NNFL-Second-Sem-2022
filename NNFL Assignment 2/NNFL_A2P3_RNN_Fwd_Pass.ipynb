{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_From_Scratch_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "IF-dv2i-E2rS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to the second NNFL assignment. In this assignment you will be programming an RNN from scratch and creating a preproccessing pipeline for Natural Language Processing. While RNNs are typically programmed using frameworks like PyTorch, the preprocessing pipeline that you will learn about here will be applicable in a lot of NLP problems you will face.\n",
        "\n",
        "Please read the instructions given below carefully before attempting the assignment.  \n",
        "- Do NOT import any other modules\n",
        "- Do NOT change the prototypes of any of the functions\n",
        "- Sample test cases are already given, test your code using these sample cases\n",
        "- Grading will be based on hidden test cases\n",
        "- Please solve this notebook using [Google Colab](https://colab.research.google.com/) as the required packages are already installed "
      ],
      "metadata": {
        "id": "oU7CZ9X2xcJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE`, as well as your name and ID number below:"
      ],
      "metadata": {
        "id": "YMonCIu5vq9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NAME = \"\"\n",
        "ID = \"\""
      ],
      "metadata": {
        "id": "4QXpkM3O8IhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing the Dataset and the GloVe embeddings"
      ],
      "metadata": {
        "id": "WDjBIyzkwT8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will kick things off by installing all the pretrained models and the dataset. Running the below cell should set you up.\n",
        "\n",
        "While glove embeddings would have been covered in class, you can find some links about them below:\n",
        "\n",
        "1. [Glove paper](https://nlp.stanford.edu/pubs/glove.pdf)\n",
        "2. [For the lazy ones](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010)"
      ],
      "metadata": {
        "id": "8K2YTcH_v6Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "! unzip glove.6B.zip\n",
        "! rm glove.6B.100d.txt glove.6B.200d.txt glove.6B.300d.txt glove.6B.zip\n",
        "! pip install --upgrade --no-cache-dir gdown\n",
        "! gdown --id 1sfQ2Y6kvmrScWMOt4c2zKYxfws5Ivu7x"
      ],
      "metadata": {
        "id": "1PZMWiSep0aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the requisite libraries. Keep in mind the ones that are imported - you will be needing them at a later point."
      ],
      "metadata": {
        "id": "scO0QLY4xUBI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSyzuofb1PXX"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement"
      ],
      "metadata": {
        "id": "grOOyzwW1URn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem we will try to solve is next word prediction. Given a sequence of words, we want to train an RNN cell to predict the next most probable word."
      ],
      "metadata": {
        "id": "XKCDcr5C1VuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell initialises all your parameters in the model. Each of these will be explained in due course of time."
      ],
      "metadata": {
        "id": "IVp2QxBdA8Gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LEN = 8\n",
        "VOCAB_SIZE = 6000\n",
        "EMB_DIM = 50\n",
        "HIDDEN_LEN = 64\n",
        "BATCH_SIZE = 64\n",
        "DATA_PATH = '/content/RNN_From_Scratch_Dataset.txt'"
      ],
      "metadata": {
        "id": "tWUhSvFAA54Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing [3.25M]"
      ],
      "metadata": {
        "id": "U43E0qrJ1RT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing of data involves the following steps for each line of your dataset:\n",
        "1. Remove all punctuation from the data. This is done so that our model does not encounter characters which will not contribute to the prediction of the next word.\n",
        "2. Convert the data into tokens - in this case the tokens would just be words. You should have a clear understanding of the difference between words and tokens.\n",
        "3. Pad the token sequence with padding tokens, or slice it depending on its length. This is done so that all your datapoints are of the same length. This allows us to ensure that when pytorch creates a batch, no errors are encountered. The inspiration for padding, however, comes from transformers. You can read more about them [here](https://arxiv.org/abs/1706.03762)."
      ],
      "metadata": {
        "id": "FQ4rQ8Ka0Frb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first write functions for each of these individual tasks. Note that each of these will be for a single datapoint."
      ],
      "metadata": {
        "id": "N2gG_oCt1s8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 0.5 Marks\n",
        "def clean_str(line):\n",
        "\t'''\n",
        "\t\tRemove punctuation marks from the input strings\n",
        "    chars_to_remove = [',', '.', '\"', \"'\", '/', '*', ',', '?', '!', '-', '\\n', '“', '”', '_', '&', '\\ufeff', '&', ';', \":\"]\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\tline: The raw text string\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tLowercased string without punctuation marks\n",
        "\t'''\n",
        "\t# YOUR CODE HERE\n",
        "\treturn line"
      ],
      "metadata": {
        "id": "aR16p2Fn1rki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Test Case\n",
        "test_str = 'Who, let* the. dogs- out?'\n",
        "assert ',' not in clean_str(test_str) and '*' not in clean_str(test_str) and '?' not in clean_str(test_str)\n",
        "print('Sample Test passed', '\\U0001F44D')"
      ],
      "metadata": {
        "id": "d9NLS0BW_5vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tokeniser is a model that splits our words into tokens. Since our problem is next word prediction, our tokeniser function tokenises the words by splitting them, therefore this is called a Word Tokeniser. You can read about other types of tokenisers [here](https://huggingface.co/docs/transformers/tokenizer_summary)"
      ],
      "metadata": {
        "id": "sGzGjMa24Suk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 0.5 Marks\n",
        "def tokenise(line):\n",
        "\t'''\n",
        "\t\tTokenise the raw string into word tokens\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\tline: The raw text string\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tTokens in the string, split at a space\n",
        "\t'''\n",
        "\t# YOUR CODE HERE\n",
        "\treturn tokens"
      ],
      "metadata": {
        "id": "GaUA4Exs1YOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Test Case\n",
        "test_str = 'Who let the dogs out\\n'\n",
        "assert tokenise(test_str) == ['Who', 'let', 'the', 'dogs', 'out']\n",
        "print('Sample Test passed', '\\U0001F44D')"
      ],
      "metadata": {
        "id": "B-_JFruDAhoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All our training points should have the same length. This is done to ensure that the pytorch dataloader can use them with minimal effort from our side.\n",
        "\n",
        "Another reason for this is that RNNs operate sequentially. Having, say, 200 tokens per training point(which might just be true for our dataset) will cause the training process to slow down. Moreover, RNNs struggle with long sequences.\n",
        "\n",
        "**Note:** Since this function works with preprocessed raw strings, we can also use it to create our labels. Therefore, before slicing/padding, ensure that you have extracted the label and have updated the training datapoint accordingly."
      ],
      "metadata": {
        "id": "6FU_fMOs35hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 0.75 Marks\n",
        "def pad_sequence(tokens, seq_len, padding_token = '<PAD>'):\n",
        "\t'''\n",
        "\t\tPadding/slicing sequences of tokens to ensure all of them have the same length. After the padding is done, the next word label is also appended to it.\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\ttokens: tokens generated from the tokenizer\n",
        "\t\t\tseq_len: The maximum permitted length of the sequence\n",
        "\t\t\tpadding_token: The token to be used in case of padding\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\ttokens: A list of padded/sliced tokens of len = max_len\n",
        "\t\t\tlast_token: The label for one datapoint, the token with the highest index\n",
        "\t'''\n",
        "\t# YOUR CODE HERE\n",
        "\treturn tokens, last_token"
      ],
      "metadata": {
        "id": "O8wbDshU3lgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Test Case\n",
        "test_seq = ['Who', 'let', 'the', 'dogs', 'out']\n",
        "tokens, last_token = pad_sequence(test_seq, SEQ_LEN)\n",
        "assert tokens == ['Who', 'let', 'the', 'dogs', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] and last_token == 'out'\n",
        "print('Sample Test passed', '\\U0001F44D')"
      ],
      "metadata": {
        "id": "7oJQaQAuA3VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the above functions should now be called from one function, which will preprocess the entire dataset."
      ],
      "metadata": {
        "id": "5GeIBn8X3s7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 1.5 Marks\n",
        "def preprocess_data(path, vocab_size, seq_len):\n",
        "\t'''\n",
        "\t\tFunction to call all preprocessing steps for the entire corpus\n",
        "\n",
        "\t\tNote: Ensure to leave a slot in the vocabulary for the <UNK> token\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\tpath: The path to your data file\n",
        "\t\t\tvocab_size: The number of tokens to be included in the vocabulary\n",
        "\t\t\tseq_len: The maximum permitted length of the token sequences\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tdatapoints: The preprocessed training + testing points for the corpus, list format\n",
        "      labels: The labels to be used per datapoint, list format\n",
        "\t'''\n",
        "\t# YOUR CODE HERE\n",
        "\n",
        "\treturn datapoints, labels"
      ],
      "metadata": {
        "id": "1zNt-wkF0FBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the below cell to preprocess your data."
      ],
      "metadata": {
        "id": "vbdWybl2BQya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datapoints, labels = preprocess_data(DATA_PATH, VOCAB_SIZE, SEQ_LEN)"
      ],
      "metadata": {
        "id": "YP52HLLGBOx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Test Case\n",
        "assert len(datapoints[0]) == len(datapoints[-1]) == len(datapoints[1231]) == SEQ_LEN\n",
        "assert len(labels) == len(datapoints)\n",
        "print('Sample Test passed', '\\U0001F44D')"
      ],
      "metadata": {
        "id": "-xD5qxXYjv0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the vocab and converting the tokens to numbers [1.25M]"
      ],
      "metadata": {
        "id": "sXoz96tj6ACJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More often than not, you will have a lot of words in your dataset - more than you require. Therefore, VOCAB_SIZE becomes a parameter that you can manually set as per your requirements.\n",
        "\n",
        "Your model cannot work with textual words - it needs numbers. For this purpose, we convert the words into numbers by creating a one-one mapping between words and a set of indices.\n",
        "\n",
        "For creating the vocabulary, we will be choosing the top-k words(k = user-defined vocabulary size) in our dataset. We also need a way to work with words not in our vocab - thus comes the ```<UNK>``` token. Any word not in our vocabulary is allotted this token. This has to manually be added into the dataset.\n",
        "\n",
        "We will also create an inverse mapping which can be used for decoding the next word from our model."
      ],
      "metadata": {
        "id": "VO8Q7Cvo6FYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(datapoints, labels, vocab_size):\n",
        "\t'''\n",
        "\t\tBuilding the vocabulary from the most common words in the corpus\n",
        "\n",
        "\t\tNote: Ensure to leave a slot in the vocabulary for the <UNK> token. \n",
        "\t\tFor uniformity, insert this at the end of your vocab, ie, its index should be 4999.\n",
        "\t\tAlso ensure that each label is in the vocab. If not, add it by removing the least common word.\n",
        "\t\tAlso ensure you remove padding tokens from the vocabulary and add the most appropriate word.\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\tdatapoints: The preprocessed datapoints in the corpus\n",
        "\t\t\tlabels: The labels per datapoint in the corpus\n",
        "\t\t\tvocab_size: The number of tokens in the vocab\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tvocab: A dicitionary mapping from the word to its corresponding vocab index(0 indexed)\n",
        "\t\t\tvocab_inv: A dicitionary mapping from the vocab index to its corresponding word\n",
        "\t'''\t\n",
        "\t\tword_sea = []\n",
        "\tfor datapoint in datapoints + labels:\n",
        "\t\tword_sea.extend(datapoint)\n",
        "\n",
        "\tmost_common_words = [word for word, _ in Counter(word_sea).most_common(vocab_size - 1)]\n",
        "\treplaced_idx = 1\n",
        "\tfor label in labels:\n",
        "\t\tif label not in most_common_words:\n",
        "\t\t\twhile(most_common_words[-replaced_idx] in labels):\n",
        "\t\t\t\treplaced_idx += 1\n",
        "\t\t\tmost_common_words[-replaced_idx] = label\n",
        "\t\n",
        "\tvocab = {word: idx for idx, word in enumerate(most_common_words)}\n",
        "\tvocab_inv = {idx: word for idx, word in enumerate(most_common_words)}\n",
        "\tvocab['<UNK>'] = vocab_size - 1\n",
        "\tvocab_inv[vocab_size - 1] = '<UNK>'\n",
        "\treturn vocab, vocab_inv"
      ],
      "metadata": {
        "id": "7tHkfb2O1Wzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function will convert the tokens in our dataset to tokens in our created vocabulary. This means that tokens not in our vocabulary wil get mapped to ```<UNK>```"
      ],
      "metadata": {
        "id": "WISklrPR7PiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data2tokens(vocab, raw_data):\n",
        "\t'''\n",
        "\t\tConverts the raw text into their corresponding tokens\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\tvocab: Mapping from the word to its corresponding vocab index\n",
        "\t\t\traw_data: The preprocessed data, however, some words are not present as \n",
        "\t\t\t\t\t\t\t  tokens in the vocab\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tdataset_tokens: A list of the preprocessed data where all words are correspoding to \n",
        "\t\t\t\t\t\t\t\t\t    tokens in the vocab\n",
        "\t'''\n",
        "\tdataset_tokens = []\n",
        "\tfor data in raw_data:\n",
        "\t\tdataset_tokens.append([word if word in vocab else '<UNK>' for word in data])\n",
        "\t\n",
        "\treturn dataset_tokens"
      ],
      "metadata": {
        "id": "CNwRF3Y27PAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above tokens can now mapped to indices in our vocab."
      ],
      "metadata": {
        "id": "hFMDZMWm7t1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 1.25 Marks\n",
        "def tokens2ids(vocab, data_tokens):\n",
        "\t'''\n",
        "\t\tConverts the tokens into their corresponding vocab indices\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\tvocab: Mapping from the word to its corresponding vocab index\n",
        "\t\t\tdata_tokens: The preprocessed data where all words are correspoding to \n",
        "\t\t\t\t\t\t\t\t\t tokens in the vocab\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tdataset_ids: The tokens in the dataset converted to their vocab indices\n",
        "\t\t\tThis should be a Pytorch Long Tensor\n",
        "\t'''\n",
        "\t# YOUR CODE HERE\n",
        "\t\n",
        "\treturn dataset_ids"
      ],
      "metadata": {
        "id": "CE2MWnYF2WzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the below cell to call all the functions above in sequence."
      ],
      "metadata": {
        "id": "VAjmy8498B3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, vocab_inv = build_vocab(datapoints, labels, VOCAB_SIZE)\n",
        "dataset_tokens = data2tokens(vocab, datapoints)\n",
        "dataset_ids = tokens2ids(vocab, dataset_tokens)"
      ],
      "metadata": {
        "id": "ADQMNCBX8BWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Test Case\n",
        "assert len(vocab) == VOCAB_SIZE\n",
        "assert len(dataset_tokens) == len(dataset_ids)\n",
        "assert torch.is_tensor(dataset_ids)\n",
        "print('Sample Test passed', '\\U0001F44D')"
      ],
      "metadata": {
        "id": "pcfdgSEai10g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our data is ready, we need to find a way to feed it into the model. As you have learnt in the previous assignment, we use mini batch sampling for inputs into the model. PyTorch uses a dataloader class(which you used in your previous assignment) which makes this possible. This next function will be an emulation of the dataloader in vanilla Python."
      ],
      "metadata": {
        "id": "FJKBUpzy8Jcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(datapoints, labels, num_batches, batch_size):\n",
        "  '''\n",
        "    Function to create the dataloader which will yield batches on the fly.\n",
        "\n",
        "    Arguments:\n",
        "      datapoints: The preprocessed datapoints in the corpus\n",
        "      labels: The labels per datapoint\n",
        "      num_batches: The number of batches from the dataset\n",
        "      batch_size: The number of datapoints per batch\n",
        "    Returns:\n",
        "      x: One minibatch of input indices of size (batch_size, seq_len)\n",
        "      y: One minibatch of labels per datapoints of size (batch_size, 1)\n",
        "  '''\n",
        "  for i in range(num_batches):\n",
        "    if i == num_batches - 1:\n",
        "      x = datapoints[i*batch_size:]\n",
        "      y = labels[i*batch_size:]\n",
        "    else:\n",
        "      x = datapoints[i*batch_size: (i+1)*batch_size]\n",
        "      y = labels[i*batch_size: (i+1)*batch_size]\n",
        "    yield x, y"
      ],
      "metadata": {
        "id": "jrLu-dxj717y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelling [5.5M]"
      ],
      "metadata": {
        "id": "WfEKhLJI8xBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step would be to build an RNN cell. This would be randomly initialised. An RNN contains 5 matrices, each of which have been described in the docstring. You would return a dictionary with the values being the matrices and keys being their corresponding notations. \n",
        "\n",
        "The equations of an RNN can be summarised as:\n",
        "\n",
        "### $ h^{(t)} = tanh(E \\cdot I) + h^{(t - 1)} \\cdot H + I_b $\n",
        "###  $ o^{(t)} = h^{(t - 1)} \\cdot O + O_b$"
      ],
      "metadata": {
        "id": "g5anN1Pr86a3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 0.75 Marks\n",
        "def create_rnn(hidden_len, emb_dim):\n",
        "\t'''\n",
        "\t\tCreates a randomly intialised rnn cell\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\thidden_len: The length of the hidden state of the rnn\n",
        "\t\t\temb_dim: The length of the embeddings in the embedding space\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\trnn: A dictionary containing all the weights and biases associated with the rnn cell (use torch.randn).\n",
        "\t\t\t\trnn['I']: The learnable weights to convert the input embeddings to the current hidden state\n",
        "\t\t\t\trnn['H']: The learnable weights to convert the previous hidden state to the current hidden state\n",
        "\t\t\t\trnn['O']: The learnable weights to convert the current hidden state to the output vector\n",
        "\t\t\t\trnn['I_b']: The bias to be used to convert the input embeddings to the current hidden state\n",
        "\t\t\t\trnn['O_b']: The bias to be used to convert the current hidden state to the output vector\n",
        "\t'''\n",
        "\t# YOUR CODE HERE\n",
        "\t\n",
        "\treturn rnn"
      ],
      "metadata": {
        "id": "p27i3TB61mUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model input, as discussed before, is going to be a set of indices corresponding to each token in the vocabulary. This cannot be directly fed in because they do not mean anything to our model. They are not present in a common vector space. For this purpose, we create \"embeddings\" which is a multi-dimensional representation of our vocabulary. These are stored in a lookup table and are learnable features, just like the weights and biases of our network. They can be indexed using the indices we have created in our vocab.\n",
        "\n",
        "\n",
        "You have to write a function to initialise this lookup table, as per the conditions given in the docstring. The ```load_pretrained_embeddings``` loads the GloVe Embeddings for you in a dictionary mapping the GloVe tokens to GloVe embeddings."
      ],
      "metadata": {
        "id": "3PrP3gNR9fHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pretrained_embeddings(model_name):\n",
        "  '''\n",
        "\t\tReads and loads the pretrained glove embeddings from the downloaded glove file\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\tmodel_name: The path to the pretrained glove file\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tembedding_model: Mapping from token to its correpsonding glove embedding\n",
        "\t'''\n",
        "  embedding_model = {}\n",
        "  f = open(model_name, 'r')\n",
        "  for line in tqdm(f.readlines(), desc = 'Reading GloVe Embeddings'):\n",
        "    tmp = line.split(' ')\n",
        "    word, vec = tmp[0], list(map(float, tmp[1:]))\n",
        "    assert(len(vec) == 50)\n",
        "    if word not in embedding_model:\n",
        "        embedding_model[word] = torch.tensor(vec)\n",
        "        \n",
        "  return embedding_model\n",
        "\n",
        "# GRADED - 1.75 Marks\n",
        "def create_embeddings(emb_dim, num_tokens, vocab, model_name = 'glove.6B.50d.txt'):\n",
        "  '''\n",
        "\t\tCreates and initialises the embeddings for the corpus:\n",
        "    1. If a token in the corpus is present as a token in the glove embeddings, initialise it with the glove embedding\n",
        "    2. If a token in the corpus is not present as a token in the glove embeddings, initialise it with a random embedding sampled from U(-0.25, 0.25)\n",
        "    3. Initialise the padding token with a zero embedding\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\temb_dim: The length of the embeddings in the embedding space\n",
        "      num_tokens: The number of tokens in the vocabulary, aka, vocab size\n",
        "      vocab: Mapping from the word to its corresponding vocab index\n",
        "      model_name: The path to the pretrained glove file\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tembeddings: The initialised embedding space (a torch tensor)\n",
        "\t'''\n",
        "  # YOUR CODE HERE\n",
        "    \n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "j-6VnNoO1c4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function creates your classifier, which is a fully connected layer. As before, you have to return a dictionary which contains the weights and baises of the classifier. The equations of the classifier can be summarised as:\n",
        "\n",
        " ### $ Y = (X \\cdot W + b)$"
      ],
      "metadata": {
        "id": "TTGHzEc0-5cS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 0.75 Marks\n",
        "def create_classifier(in_features, num_classes):\n",
        "\t'''\n",
        "\t\tCreates a randomly intialised classifer as a fully connected layer\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\tin_features: The length of the feature vector at the input of the classifier\n",
        "\t\t\tnum_classes: The number of classes to be predicted\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tclassifier: \n",
        "\t\t\t\tclassifier['weight']: The randomly initialised weights for the fully connected layer from in_features to num_classes (use torch.randn)\n",
        "\t\t\t\tclassifier['bias']: The randomly initialised bias for the fully connected layer from in_features to num_classes\n",
        "\t'''\n",
        "\t# YOUR CODE HERE\n",
        "\treturn classifier"
      ],
      "metadata": {
        "id": "7nzd7LvA1VJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function forwards the rnn by one step for all elements in the batch. In case a padding token is encountered, no change is made to the output and the hidden state. For this purpose, you have been provided the previous hidden state and the previous output as an input into the function.\n",
        "\n",
        "### $ h^{(t)} = tanh(E \\cdot I) + h^{(t - 1)} \\cdot H + I_b $\n",
        "###  $ o^{(t)} = h^{(t - 1)} \\cdot O + O_b$"
      ],
      "metadata": {
        "id": "u4mHywfl_aH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 0.75 Marks\n",
        "def forward_rnn_one_step(rnn, embs, prev_hidden, prev_output):\n",
        "\t'''\n",
        "\t\tTakes one forward step through the rnn cell. In case a padding token is encountered, no change is made to the output and the hidden state.\n",
        "\t\t\n",
        "\t\tArguments:\n",
        "\t\t\trnn: Dictionary containing the weights and biases of the rnn\n",
        "\t\t\tembs: The input embeddings of the tokens of the datapoint\n",
        "\t\t\tprev_hidden: The previous hidden state\n",
        "\t\t\tprev_output: The previous outputs\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\thidden_state: The next hidden state\n",
        "\t\t\toutput: The output for this sequence\n",
        "\t'''\n",
        "\t# YOUR CODE HERE\n",
        "\n",
        "\treturn hidden_state, output"
      ],
      "metadata": {
        "id": "5GQRtbVU1ep6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below function passes your features through the full connected layer. The equations are summarised again for your convenience:\n",
        "\n",
        " $ Y = (X \\cdot W + b)$"
      ],
      "metadata": {
        "id": "2NFRrShZ_0lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 0.75 Marks\n",
        "def classify(feat, classifier):\n",
        "\t'''\n",
        "\t\tPerforms a forward pass through the classifier\n",
        "\n",
        "\t\tArguments:\n",
        "\t\t\tfeat: The feature vector for classification\n",
        "\t\t\tclassifier: Dictionary containing the weights and biases of the classifier\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tlogits: The logits for each word in our model\n",
        "\t'''\n",
        "\t# YOUR CODE HERE\n",
        "\treturn logits"
      ],
      "metadata": {
        "id": "zAKtvg8V1g7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The forward function passes your input data through all the above functions in sequence. Note the sequential nature of calling the rnn, since the previous hidden state has to be used.\n",
        "\n",
        "The initial hidden state has to be initialised to zeros, while the output has to be randomly initialised.\n",
        "\n",
        "The features to be used for the classifier is the final output of the RNN. Note that the features can be a concatenations of all outputs/hidden states too. You will have to change the classifier accordingly. \n",
        "\n",
        "The output logits will have to be converted to a probability distribution. For this purpose, it will be passed through a softmax activation."
      ],
      "metadata": {
        "id": "SxbdiDe0AFPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(x, seq_len, hidden_len, classifier, embs, rnn):\n",
        "\t'''\n",
        "\t\tPerforms a foraward pass for the batched data\n",
        "\n",
        "\t\tArguments:\n",
        "\t\t\tx: The feature vector for classification\n",
        "\t\t\tseq_len: The maximum permitted length of the token sequences in the datapoints\n",
        "\t\t\thidden_len: The length of the hidden state of the rnn cell\n",
        "\t\t\tclassifier: Dictionary containing the weights and biases of the classifier\n",
        "\t\t\tembs: The input embeddings of the tokens of the datapoint\n",
        "\t\t\trnn: Dictionary containing the weights and biases of the rnn\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t\tprobs: The probabilities of all the next words\n",
        "\t'''\n",
        "\tinput_embs = embs[x]\n",
        "\thidden_state = torch.zeros(x.shape[0], hidden_len)\n",
        "\toutput = torch.zeros(x.shape[0], hidden_len)\n",
        "\tfor i in range(seq_len):\n",
        "\t\thidden_state, output = forward_rnn_one_step(rnn, input_embs[:, i, :], hidden_state, output)\n",
        "\t\t\n",
        "\tlogits = classify(output, classifier)\n",
        "\tprobs = F.softmax(logits, dim = 1)\n",
        "\treturn probs"
      ],
      "metadata": {
        "id": "7x9Pma-P1j1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the below cell to call your modelling functions."
      ],
      "metadata": {
        "id": "Q28VzfXeBbNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(69)\n",
        "print('Creating RNN')\n",
        "rnn = create_rnn(HIDDEN_LEN, EMB_DIM)\n",
        "print('Creating Embeddings')\n",
        "embs = create_embeddings(EMB_DIM, VOCAB_SIZE, vocab)\n",
        "print('Creating Classifier')\n",
        "classifier = create_classifier(HIDDEN_LEN, VOCAB_SIZE)"
      ],
      "metadata": {
        "id": "fNN-R2zRXNBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Test Cases\n",
        "assert rnn['H'].shape == (HIDDEN_LEN, HIDDEN_LEN)\n",
        "assert torch.isclose(rnn['I_b'][40], torch.tensor(0.1261), atol = 0.1) \n",
        "assert torch.isclose(rnn['I'][40][40], torch.tensor(0.9098), atol = 0.1) \n",
        "assert embs.shape == (VOCAB_SIZE, EMB_DIM)\n",
        "assert len(classifier['bias']) == VOCAB_SIZE\n",
        "print('Sample Test passed', '\\U0001F44D')"
      ],
      "metadata": {
        "id": "9oyK7TFckq5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Test Cases\n",
        "torch.manual_seed(69)\n",
        "test_output, _ = forward_rnn_one_step(rnn, torch.randn(32, EMB_DIM), torch.zeros(32, HIDDEN_LEN), torch.zeros(32, HIDDEN_LEN))\n",
        "assert torch.isclose(test_output[1][0], torch.tensor(0.9958), atol = 0.01)\n",
        "print('Sample Test passed', '\\U0001F44D')"
      ],
      "metadata": {
        "id": "NAFTEyePFCYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that your data and model is ready, it is time to pass it through the model and get some predictions. Write an expression to calculate the number of batches and use that to create your dataloader, using the variables and prototype you have created above."
      ],
      "metadata": {
        "id": "3pd4vVHDBvR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED - 0.75 Marks\n",
        "num_batches = # YOUR CODE HERE\n",
        "dataloader = create_dataloader(dataset_ids, labels, num_batches, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "CInFhfcsBOdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like you do in PyTorch, loop through your dataloader to get the batches. Perform a forward pass to get the probabilities of your next words. Choose the most probable word using ```torch.argmax``` and add these to a list called ```preds```.\n",
        "\n",
        "These predictions will be indices, therefore, use ```vocab_inv``` to convert it back into words."
      ],
      "metadata": {
        "id": "GQIsUl-8B9wN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "for x, y in tqdm(dataloader, total = num_batches, desc = 'Forward Pass'):\n",
        "  probs = forward(x, SEQ_LEN, HIDDEN_LEN, classifier, embs, rnn)\n",
        "  next_word = torch.argmax(probs, dim = 1)\n",
        "  preds.extend([vocab_inv[int(word)] for word in next_word])"
      ],
      "metadata": {
        "id": "ZokoQcq-p8gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Test Case\n",
        "assert len(preds) == len(datapoints)\n",
        "print('Sample Test passed', '\\U0001F44D')"
      ],
      "metadata": {
        "id": "YfbJPR2mhN2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ETYZ2G1L6k_"
      },
      "source": [
        "# End of this part.\n",
        "Assignment by:\n",
        "\n",
        "Devaansh Gupta (f20190187@pilani.bits-pilani.ac.in)\n",
        "\n",
        "Palaash Agrawal (f20180565@pilani.bits-pilani.ac.in)\n",
        "\n",
        "Harsh Sulakhe (f20180186@pilani.bits-pilani.ac.in)\n",
        "\n"
      ]
    }
  ]
}